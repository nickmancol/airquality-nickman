{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.svm import SVC\n",
    "import scipy.stats  as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#from sklearn.svm\n",
    "%matplotlib inline\n",
    "\n",
    "db_string = \"postgres://rossi:123456@localhost:5432/air_quality\"\n",
    "db = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(folder='./', preffix='mi_pollution', encoding='ISO-8859-1'):\n",
    "    # Joins all the preffix*.csv files and the data from stations of the preffix_legend-mi.csv\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"folder {folder} doesn't exist's, no data merged\")\n",
    "        return\n",
    "\n",
    "    dataframes = []\n",
    "    names = pd.DataFrame()\n",
    "    for filename in os.listdir(folder):\n",
    "        file = folder+filename\n",
    "        if 'legend' in filename and filename.endswith(\".csv\"):\n",
    "            names = pd.read_csv(file, header=None, encoding=encoding)\n",
    "            names.rename(columns=pd.to_numeric)\n",
    "        elif filename.startswith(preffix) and filename.endswith(\".csv\"):\n",
    "            if csv.Sniffer().has_header(file):\n",
    "                df = pd.read_csv(file, header=None, skiprows=[0], encoding=encoding)\n",
    "            else: \n",
    "                df = pd.read_csv(file, encoding=encoding)\n",
    "            \n",
    "            dataframes.append(df)\n",
    "\n",
    "    total = pd.DataFrame()\n",
    "    for df in dataframes:\n",
    "        total = total.append(df)\n",
    "\n",
    "    total = total.drop_duplicates()\n",
    "    total = pd.merge(total, names, left_on=0, right_on=0, how='inner')\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumes no header in the names file\n",
    "path = '../MI_Air_Quality/data/'\n",
    "air = merge_data(path)\n",
    "air.rename(columns={0:'sensor_id','1_x': 'date_time', '2_x': 'val', '1_y':'station_name', '2_y':'latitude'\n",
    "                    , 3:'longitude', 4:'particle', 5:'unit',6:'date_format'}, inplace=True)\n",
    "air.info()\n",
    "air.to_csv('./data/air_complete.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumes no header in the names file\n",
    "path = '../MI_Weather_Station_Data/data/'\n",
    "weather = merge_data(path, preffix='mi_meteo_')\n",
    "weather.rename(columns={0:'sensor_id','1_x': 'date_time', '2_x': 'val', '1_y':'station_name', '2_y':'latitude'\n",
    "                    , 3:'longitude', 4:'type', 5:'unit'}, inplace=True)\n",
    "weather.info()\n",
    "weather.to_csv('./data/weather_complete.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air = pd.read_sql('select * from vw_cross_air', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df_missing(ax, df):\n",
    "    n_df_sensors = np.arange(len(df.columns)-1)\n",
    "    list_missing = []\n",
    "    total_hours = len(df)\n",
    "    for c in df.columns:\n",
    "        if c.isdigit(): \n",
    "            list_missing.append(df[c].isnull().sum())\n",
    "\n",
    "    ax.bar(n_df_sensors, [total_hours-x for x in list_missing], bottom=list_missing, label='Available')\n",
    "    ax.bar(n_df_sensors, list_missing, label='Missing')\n",
    "    ax.axhline(total_hours/2, xmin=0, xmax=len(n_df_sensors), c='w')\n",
    "    return n_df_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "plot_df_missing(ax, df_air)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(len(df_air.columns)-1), df_air.columns[1:], rotation=90)\n",
    "plt.title('Air values per sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distrib(a_df, b_df):\n",
    "    bins = 100\n",
    "    num_plots = len(a_df.columns)-1\n",
    "    fig, ax = plt.subplots(num_plots, 1, figsize=(10,6*num_plots))\n",
    "    for idx, c in enumerate(a_df.columns):\n",
    "        if c.isdigit(): \n",
    "            vals = a_df[a_df[c].notna()][c].values\n",
    "            ax[idx-1].hist(vals, bins=bins, label=c)\n",
    "            ax[idx-1].hist(b_df[c], alpha=0.5, bins=bins, label=c)\n",
    "            ax[idx-1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negative(x, vals=None, neg_allowed=False):\n",
    "    res = x\n",
    "    mx = max(vals)\n",
    "    mi = min(vals)\n",
    "    if x > mx:\n",
    "        res = np.random.uniform(mi, np.median(vals))\n",
    "    elif (not neg_allowed) & (x <= 0): \n",
    "        res = mi\n",
    "    elif neg_allowed & (x < mi): \n",
    "        res = np.random.uniform(0, mi)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def interpolate_df(df, neg_allowed=False):\n",
    "    int_df = pd.DataFrame()\n",
    "    total_hours = len(df)\n",
    "    for c in df.columns:\n",
    "        original_vals = sorted(df[df[c].notna()][c].values[:], reverse=True)\n",
    "        try:\n",
    "            int_df[c] = df[c]\n",
    "            g = df.groupby(df['date_time'].dt.normalize())\n",
    "            for name, group in g:\n",
    "                if group[c].count() == 1:\n",
    "                    a = int_df[(int_df['date_time'].dt.normalize() == name) & (df[c].notna())][c]\n",
    "                    int_df.loc[(int_df['date_time'].dt.normalize() == name) & (df[c].isna()), c] = a.values[0]\n",
    "            missing = total_hours - df[c].count()\n",
    "            while missing > 0:\n",
    "                int_df[c] = int_df[c].interpolate(method='spline', order=3, limit_direction='both', limit=3)\n",
    "                int_df[c] = int_df[c].apply(lambda x: replace_negative(x, original_vals, neg_allowed=neg_allowed))\n",
    "                missing = total_hours - int_df[c].count()\n",
    "        except RuntimeError:\n",
    "            print(f\"{c} {RuntimeError}\")\n",
    "    return int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date_sensor_val(df, sensor_id, dest_date_time, date_src):\n",
    "    date_time_src = date_src.replace(hour=dest_date_time.hour)\n",
    "    val = df[(df['date_time'] == date_time_src)][sensor_id].values[0]\n",
    "    return val\n",
    "    \n",
    "def create_interpolate_sampling(df, sensors):\n",
    "    return df.drop(sensors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_sampled = df_air.copy()\n",
    "df_air_sampled = create_interpolate_sampling(df_air_sampled, ['5552','17127','20004','20020'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_int = interpolate_df(df_air_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6))\n",
    "ax.plot(df_air['date_time'],df_air['20005'], label='air')\n",
    "ax.plot(df_air_sampled['date_time'], df_air_sampled['20005'], alpha=0.5, label='sampled')\n",
    "ax.plot(df_air_int['date_time'],df_air_int['20005'], label='interpolated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distrib(df_air_sampled, df_int_air)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_sql('select * from vw_cross_weather', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,6))\n",
    "plot_df_missing(ax, df_weather)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(len(df_weather.columns)-1), df_weather.columns[1:], rotation=90)\n",
    "plt.title('Weather values per sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_sampled = df_weather.copy()\n",
    "df_weather_sampled = create_interpolate_sampling(df_weather_sampled, ['9341','14391','19004','19005','19006','19019','19021'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_graph = '6030'\n",
    "df_weather_int = interpolate_df(df_weather_sampled, neg_allowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,6))\n",
    "ax.plot(df_weather['date_time'],df_weather[s_to_graph], label='air')\n",
    "ax.plot(df_weather_sampled['date_time'], df_weather_sampled[s_to_graph], alpha=0.5, label='sampled')\n",
    "ax.plot(df_weather_int['date_time'],df_weather_int[s_to_graph], label='interpolated')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distrib(df_weather_sampled, df_weather_int)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adds average per day per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_measures = pd.read_sql('''select case a.particle when 'Ozone' then 'Ozono' else a.particle end measure, a.unit, sensor_id\n",
    "from air_complete a\n",
    "group by a.particle, a.unit, sensor_id\n",
    "order by a.particle desc;''', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_measures = pd.read_sql('''\n",
    "select w.\"type\" measure, w.unit, sensor_id\n",
    "from weather_complete w\n",
    "group by w.\"type\", w.unit, sensor_id\n",
    "order by w.\"type\" desc;''', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_df_measures(df, df_measures):\n",
    "    res = pd.DataFrame({'date_time':df.loc[:, 'date_time'].values})\n",
    "    for m in df_measures.measure.unique():\n",
    "        if m != 'Wind Direction':\n",
    "            sensors = [str(s) for s in df_measures.loc[df_measures['measure']==m, 'sensor_id'].unique()]\n",
    "            if set(sensors).intersection(df.columns):\n",
    "                res[m] = df.loc[:, sensors].mean(axis=1).values\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_weather = avg_df_measures(df_weather_int, df_weather_measures)\n",
    "df_avg_air = avg_df_measures(df_air_int, df_air_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_air.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic and car data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vehicles(ax, df):\n",
    "    cols = df.columns.unique()\n",
    "    dt_range = pd.date_range('2013-01-01 00:00', '2013-12-31 23:00', freq='8H')\n",
    "    for c in cols:\n",
    "        if c != 'date_time':\n",
    "            ax.plot(df.loc[df['date_time'].isin(dt_range)].date_time, df.loc[ df['date_time'].isin(dt_range), c], label=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic_count = pd.read_sql('''select * from vw_cross_traffic;''', db)\n",
    "df_traffic_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_euro = pd.read_sql('select * from vw_cross_vehicles_euro', db).fillna(0)\n",
    "df_vehicles_vtype = pd.read_sql('select * from vw_cross_vehicles_vtype', db).fillna(0)\n",
    "df_vehicles_ftype = pd.read_sql('select * from vw_cross_vehicles_ftype', db).fillna(0)\n",
    "df_vehicles_ltype = pd.read_sql('select * from vw_cross_vehicles_ltype', db).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_euro.drop('euro_0', inplace=True, axis=1)\n",
    "df_vehicles_euro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "plot_vehicles(ax, df_vehicles_euro)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Vehicles Euro categories per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_vtype.drop('vtype_0', inplace=True, axis=1)\n",
    "df_vehicles_vtype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "plot_vehicles(ax, df_vehicles_vtype)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Vehicles VType categories per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_ftype.drop('ftype_0', inplace=True, axis=1)\n",
    "df_vehicles_ftype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "plot_vehicles(ax, df_vehicles_ftype)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Vehicles FType categories per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_ltype.drop('ltype_0', inplace=True, axis=1)\n",
    "df_vehicles_ltype.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "plot_vehicles(ax, df_vehicles_ltype)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('Vehicles LType categories per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_dpf = pd.read_sql('select * from vw_cross_vehicles_dpf', db)\n",
    "df_vehicles_dpf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr(df1, df2):\n",
    "    df_merged = df1.join(df2.set_index('date_time'), on='date_time')\n",
    "    return df_merged, df_merged.corr()\n",
    "\n",
    "def plot_corr(ax, corrs):\n",
    "    cmap = cm.OrRd\n",
    "    cax = ax.imshow(corrs, interpolation=\"nearest\", cmap=cmap)\n",
    "    ax.grid(True)\n",
    "    labels=corrs.columns\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    return cax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compulsory_cols = ['date_time', 'PM10 (SM2005)','Ozono','Nitrogene Dioxide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Ipm(df_src, vrif=50):\n",
    "    df = pd.DataFrame(df_src)\n",
    "    df['Ipm'] = np.zeros(len(df))\n",
    "    date_range = pd.date_range(min(df['date_time']), max(df['date_time']), freq='1H')\n",
    "    for d in date_range:\n",
    "        init_date = d - pd.to_timedelta(24, unit='H')\n",
    "        if init_date in date_range:\n",
    "            val = df.loc[(df['date_time'] >= init_date) & (df['date_time'] < d), 'PM10 (SM2005)'].mean()\n",
    "            df.loc[df['date_time'] == d, 'Ipm'] = val*100/vrif\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm = calc_Ipm(df_avg_air[compulsory_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_INO2(df, vrif=200):\n",
    "    df['Ino2'] = df['Nitrogene Dioxide']*100/vrif\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm = calc_INO2(df_ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_IO3(df_src, vrif=120):\n",
    "    df = pd.DataFrame(df_src)\n",
    "    df['Io3'] = np.zeros(len(df))\n",
    "    date_range = pd.date_range(min(df['date_time']), max(df['date_time']), freq='1H')\n",
    "    for d in date_range:\n",
    "        init_date = d - pd.to_timedelta(8, unit='H')\n",
    "        if init_date in date_range:\n",
    "            val = max(df.loc[(df['date_time'] >= init_date) & (df['date_time'] < d), 'Ozono'])\n",
    "            df.loc[df['date_time'] == d, 'Io3'] = val*100/vrif\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm = calc_IO3(df_ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_Iqa(df_src):\n",
    "    df = pd.DataFrame(df_src)\n",
    "    df['Iqa'] = np.zeros(len(df))\n",
    "    date_range = pd.date_range(min(df['date_time']), max(df['date_time']), freq='1H')\n",
    "    for d in date_range:\n",
    "        row = df.loc[df['date_time'] == d, ['Ipm','Ino2','Io3']]\n",
    "        df.loc[df['date_time'] == d, 'Iqa'] = (row.Ipm.values[0] + max([row.Ino2.values[0],row.Io3.values[0]]))/2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm = calc_Iqa(df_ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iqa_cats = pd.DataFrame([\n",
    "    {'min':0, 'max':50, 'cat':1}\n",
    "    ,{'min':50, 'max':75, 'cat':2}\n",
    "    ,{'min':75, 'max':100, 'cat':3}\n",
    "    ,{'min':100, 'max':125, 'cat':4}\n",
    "    ,{'min':125, 'max':150, 'cat':5}\n",
    "    ,{'min':150, 'max':175, 'cat':6}\n",
    "    ,{'min':175, 'max':50000, 'cat':7}\n",
    "])\n",
    "\n",
    "def Iqa_cat(df_src):\n",
    "    df = pd.DataFrame(df_src)\n",
    "    df['Iqa_cat'] = np.zeros(len(df))\n",
    "    date_range = pd.date_range(min(df['date_time']), max(df['date_time']), freq='1H')\n",
    "    for d in date_range:\n",
    "        val = df.loc[df['date_time'] == d, 'Iqa'].values[0]\n",
    "        cat = df_iqa_cats.loc[(df_iqa_cats['min'] <= val) & (df_iqa_cats['max'] > val)\n",
    "                              , 'cat'].values[0]\n",
    "        df.loc[df['date_time'] == d, 'Iqa_cat'] = cat\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm = Iqa_cat(df_ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ipm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = df_ipm.loc[:,['date_time','Ipm','Ino2','Iqa','Iqa_cat']]\n",
    "merged, df_corr = calc_corr(y_df, df_traffic_count)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "cax = plot_corr(ax, df_corr)\n",
    "plt.xticks(rotation=90)\n",
    "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_traffic_count.loc[:, df_traffic_count.columns != 'date_time']\n",
    "y = df_ipm['Iqa_cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_clf(classifiers, X, y):\n",
    "    res = []\n",
    "    for c in classifiers:\n",
    "        res.append(c.fit(X, y))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = fit_clf([LogisticRegression(), MultinomialNB(), RandomForestClassifier()]\n",
    "                      , X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clf(classifiers, X, y):\n",
    "    res = {}\n",
    "    np.set_printoptions(precision=2)\n",
    "    for c in classifiers:\n",
    "        y_pred = c.predict(X)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        prf = precision_recall_fscore_support(y, y_pred, beta=0.5, average=None)\n",
    "        scores = cross_val_score(c, X, y, cv=5)\n",
    "        res[type(c).__name__] = {'acc':\"%0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "                   , 'cm':cm, 'cm_norm':cm_normalized, 'pre_rec_fsc':prf, 'y_pred':y_pred}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_clf(classifiers, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(y, results):\n",
    "    num_classifiers = len(results.keys())\n",
    "    for idx, k in enumerate(results.keys()):\n",
    "        c = results[k]\n",
    "        print('****',k)\n",
    "        print('Accuracy', c['acc'])\n",
    "        print('Precision', \"%0.2f\" % (c['pre_rec_fsc'][0].mean()))\n",
    "        print('Recall', \"%0.2f\" % (c['pre_rec_fsc'][1].mean()))\n",
    "        print('F-score', \"%0.2f\" % (c['pre_rec_fsc'][1].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_test, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicles_total = pd.read_sql(\"\"\"select date_time, sum(c) total\n",
    "from vw_count_dist_vehicles\n",
    "group by date_time\n",
    "order by date_time;\"\"\", db)\n",
    "merged, df_corr = calc_corr(y_df, df_vehicles_total)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged, df_corr = calc_corr(y_df, df_vehicles_dpf)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged, df_corr = calc_corr(y_df, df_vehicles_euro)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged, df_corr = calc_corr(y_df, df_vehicles_ftype)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged, df_corr = calc_corr(y_df, df_vehicles_vtype)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged, df_corr = calc_corr(y_df, df_vehicles_ltype)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_vehicles_total.loc[:, ['total']]\n",
    "X = pd.concat([X, df_vehicles_dpf.loc[:, ['dpf_1','dpf_2']]], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_euro.loc[:, df_vehicles_euro.columns != 'date_time']], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_ftype.loc[:, df_vehicles_ltype.columns != 'date_time']], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_ltype.loc[:, df_vehicles_ltype.columns != 'date_time']], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_vtype.loc[:, df_vehicles_vtype.columns != 'date_time']], axis=1)\n",
    "#medium_vehicles\n",
    "X = pd.concat([X, df_vehicles_euro.loc[:, ['euro_6','euro_7']]], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_ftype.loc[:, ['ftype_5']]], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_vtype.loc[:, ['vtype_3']]], axis=1)\n",
    "#X = pd.concat([X, df_vehicles_ltype.loc[:, ['ltype_4_6']]], axis=1)\n",
    "y = df_ipm['Iqa_cat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model_1 = fit_clf([LogisticRegression(), MultinomialNB(), RandomForestClassifier()]\n",
    "                      , X_train, y_train)\n",
    "results_1 = compare_clf(model_1, X_test, y_test)\n",
    "plot_results(y_test, results_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
